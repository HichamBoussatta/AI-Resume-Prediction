!pip install joblib
!pip install -r requirements.txt
import streamlit as st
import pandas as pd
import joblib
import re
import nltk
import spacy
import pdfplumber
import docx
from io import StringIO
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go
from bs4 import BeautifulSoup
import requests
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter

# T√©l√©charger les stopwords
nltk.download("stopwords")

# Charger le mod√®le de langage fran√ßais de SpaCy
nlp = spacy.load("fr_core_news_sm")

# üìå Fonction pour extraire le texte d'un fichier PDF, DOCX ou TXT
def extract_text(uploaded_file):
    if uploaded_file.type == "application/pdf":
        with pdfplumber.open(uploaded_file) as pdf:
            return " ".join([page.extract_text() for page in pdf.pages if page.extract_text()])
    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(uploaded_file)
        return " ".join([para.text for para in doc.paragraphs])
    else:
        return StringIO(uploaded_file.getvalue().decode("utf-8")).read()

# üìå Fonction de nettoyage du texte
def clean_text(text):
    text = text.lower()
    text = re.sub(r"\d+", "", text)  # Supprimer les chiffres
    text = re.sub(r"[^\w\s]", "", text)  # Supprimer la ponctuation
    text = " ".join([word for word in text.split() if word not in stopwords.words("french")])
    return text

# üìå Fonction pour extraire les technologies du CV
def extract_technologies(text):
    tech_keywords = ["python", "sql", "spark", "aws", "machine learning", "deep learning",
                     "tableau", "power bi", "docker", "kubernetes", "terraform", "ci/cd"]
    return [tech for tech in tech_keywords if tech in text.lower()]

# üìå Fonction pour d√©tecter le niveau d'exp√©rience
def detect_experience(cv_text):
    cv_text = cv_text.lower()
    match = re.search(r'(\d+)\s*(ans|years)', cv_text)
    years_of_experience = int(match.group(1)) if match else 0

    junior_keywords = ["d√©butant", "junior", "stage", "apprentissage", "assistant"]
    senior_keywords = ["confirm√©", "exp√©rience", "middle", "3 ans", "4 ans", "5 ans", "6 ans", "7 ans"]
    expert_keywords = ["lead", "manager", "expert", "architecte", "10 ans", "15 ans", "principal"]

    if any(word in cv_text for word in expert_keywords) or years_of_experience >= 8:
        return "Expert"
    elif any(word in cv_text for word in senior_keywords) or 3 <= years_of_experience < 8:
        return "Senior"
    else:
        return "Junior"

# üìå Fonction pour cr√©er un nuage de mots
def generate_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt)

# üìå Fonction pour g√©n√©rer un graphique radar des comp√©tences
def plot_radar_chart(user_skills, ideal_profile):
    labels = list(set(user_skills.keys()).union(set(ideal_profile.keys())))
    user_values = [user_skills.get(skill, 0) for skill in labels]
    ideal_values = [ideal_profile.get(skill, 0) for skill in labels]
    
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(r=user_values, theta=labels, fill='toself', name='CV'))
    fig.add_trace(go.Scatterpolar(r=ideal_values, theta=labels, fill='toself', name='Profil Id√©al'))
    
    fig.update_layout(polar=dict(radialaxis=dict(visible=True)), showlegend=True)
    st.plotly_chart(fig)

# üìå Fonction pour scraper l'offre d'emploi
def scrape_job_description(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extraire les sections pertinentes
        description_section = soup.find('div', class_='job-description')
        qualifications_section = soup.find('div', class_='job-qualifications')
        criteria_section = soup.find('ul', class_='arrow-list')
        
        # Concatenation des sections
        description = ""
        if description_section:
            description += description_section.get_text(separator="\n", strip=True) + "\n"
        if qualifications_section:
            description += qualifications_section.get_text(separator="\n", strip=True) + "\n"
        if criteria_section:
            description += criteria_section.get_text(separator="\n", strip=True) + "\n"
        
        return description
    else:
        return ""

# üìå Fonction pour obtenir les mots les plus fr√©quents dans un texte
def get_top_keywords(text, num_keywords=5):
    words = text.split()
    words = [word for word in words if word not in stopwords.words("french")]  # Supprimer les stopwords
    word_counts = Counter(words)
    return word_counts.most_common(num_keywords)

# üìå Fonction pour cr√©er un nuage de mots √† partir des mots les plus fr√©quents
def generate_top_keywords_wordcloud(text):
    # Extraire les 5 mots les plus fr√©quents
    top_keywords = get_top_keywords(text, num_keywords=5)
    
    # Cr√©er un dictionnaire avec les mots et leurs fr√©quences
    word_freq = dict(top_keywords)

    # G√©n√©rer le nuage de mots avec les fr√©quences
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(word_freq)

    # Affichage du nuage de mots
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt)

# üìå Fonction pour comparer les textes
def compare_texts(text1, text2):
    # Vectorisation des textes
    vectorizer = CountVectorizer().fit_transform([text1, text2])
    cosine_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])
    
    return cosine_sim[0][0]

# üìå Fonction pour pr√©dire la cat√©gorie et l'exp√©rience en utilisant le mod√®le pr√©alablement entra√Æn√©
def predict_cv(cv_text):
    model_category = joblib.load("/content/drive/MyDrive/model_category.pkl")
    vectorizer = joblib.load("/content/drive/MyDrive/tfidf_vectorizer.pkl")
    label_encoder = joblib.load("/content/drive/MyDrive/label_encoder.pkl")

    cleaned_text = clean_text(cv_text)
    features = vectorizer.transform([cleaned_text])

    predicted_category_encoded = model_category.predict(features)[0]
    predicted_category = label_encoder.inverse_transform([predicted_category_encoded])[0]
    predicted_experience = detect_experience(cv_text)

    return predicted_category, predicted_experience

# üìå Interface Streamlit
st.title("üìÑüí° Classification Automatique des CVs")
st.subheader("Chargez un CV et obtenez la pr√©diction du m√©tier et de l'exp√©rience.")

uploaded_file = st.file_uploader("üì§ Importer un fichier (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])

# üî• L'utilisateur peut d√©finir son propre profil id√©al
st.subheader("üîß D√©finissez votre profil id√©al")
available_skills = ["python", "sql", "spark", "aws", "machine learning", "deep learning", "tableau", "power bi", "docker", "kubernetes", "terraform", "ci/cd"]
selected_skills = st.multiselect("S√©lectionnez les comp√©tences cl√©s :", available_skills)
ideal_profile = {skill: 1 for skill in selected_skills}

job_url = st.text_input("Entrez l'URL de l'offre d'emploi :", "")

if uploaded_file:
    with st.spinner("üîç Analyse en cours..."):
        cv_text = extract_text(uploaded_file)
        cleaned_cv_text = clean_text(cv_text)

        st.subheader("üîç Contenu du CV :")
        st.text_area("Texte extrait :", cv_text, height=200)

        st.subheader("‚òÅÔ∏è Nuage de mots :")
        generate_wordcloud(cleaned_cv_text)

        category, experience = predict_cv(cv_text)
        st.subheader(f"üéØ M√©tier : **{category}**")
        st.subheader(f"üìä Niveau d'exp√©rience : **{experience}**")

        technologies = extract_technologies(cv_text)
        st.subheader(f"üõ†Ô∏è Technologies d√©tect√©es : {', '.join(technologies) if technologies else 'Aucune'}")

        # üî• Ajout du graphique radar
        st.subheader("üìä Visualisation des comp√©tences")
        user_skills = {tech: 1 for tech in technologies}
        plot_radar_chart(user_skills, ideal_profile)

        # Scraper l'offre d'emploi
        job_description = scrape_job_description(job_url)
        
        if job_description:
            st.subheader("üîç Nuage de mots des 5 mots les plus fr√©quents dans l'offre d'emploi :")
            job_description_cleaned = clean_text(job_description)
            generate_top_keywords_wordcloud(job_description_cleaned)
            
            # Comparer le CV avec l'offre d'emploi
            similarity_score = compare_texts(cv_text, job_description)
            st.subheader("R√©sultats de la comparaison :")
            st.write(f"Score de similarit√© entre le CV et l'offre d'emploi : {similarity_score:.4f}")
            
            # Interpr√©tation du score de similarit√©
            if similarity_score > 0.8:
                st.success("Le CV est tr√®s similaire √† l'offre d'emploi.")
            elif similarity_score > 0.5:
                st.warning("Le CV est mod√©r√©ment similaire √† l'offre d'emploi.")
            else:
                st.error("Le CV est peu similaire √† l'offre d'emploi.")
        else:
            st.error("Erreur lors du scraping de l'offre d'emploi.")
