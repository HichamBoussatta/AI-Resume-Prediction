import streamlit as st
import joblib
import os
import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import nltk
import spacy
import pdfplumber
import docx
from io import StringIO
from wordcloud import WordCloud
import numpy as np
import plotly.graph_objects as go
from bs4 import BeautifulSoup
import requests
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from datetime import datetime
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
# Importations n√©cessaires
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
import shutil
import gdown

# --- Fonction d'authentification ---
def authenticate_user():
    st.sidebar.title("Login")
    username = st.sidebar.text_input("Nom d'utilisateur", "")
    password = st.sidebar.text_input("Mot de passe", type="password")

    # Ajouter un utilisateur et un mot de passe pour la d√©monstration (√† s√©curiser davantage dans une vraie application)
    correct_username = "admin"
    correct_password = "123"

    error_message_shown = False  # Flag pour v√©rifier si l'erreur doit √™tre affich√©e

    if username and password:  # V√©rifier si les deux champs sont remplis
        if username != correct_username or password != correct_password:
            error_message_shown = True  # Marquer l'erreur pour qu'elle soit affich√©e apr√®s la tentative

    # Si l'utilisateur essaie de se connecter avec de mauvaises informations
    if error_message_shown:
        st.sidebar.error("Nom d'utilisateur ou mot de passe incorrect.")

    # V√©rification des informations de connexion
    if username == correct_username and password == correct_password:
        return True
    else:
        return False


# --- Script 1: Resume Classification Model Training ---
def resume_classification():
    nltk.download("stopwords")
    from nltk.corpus import stopwords
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

    # Stopwords en fran√ßais et en anglais
    french_stop_words = set(stopwords.words("french"))
    english_stop_words = ENGLISH_STOP_WORDS  # Stopwords en anglais
    all_stop_words = french_stop_words.union(english_stop_words)

    # Fonction avanc√©e de nettoyage du texte
    def clean_text(text):
        if pd.isna(text):  # G√©rer les valeurs NaN
            return ""
        text = text.lower()
        text = re.sub(r"[^a-zA-Z\s]", "", text)  # Supprimer les caract√®res sp√©ciaux et chiffres
        text = re.sub(r"\s+", " ", text).strip()  # Supprimer les espaces suppl√©mentaires
        text = " ".join([word for word in text.split() if word not in all_stop_words])  # Supprimer les stopwords
        return text


    # T√©l√©charger et charger le CSV
    file_url = "https://drive.google.com/uc?id=1-5Hw-uq7-NFJjcU7LuD_pgK42yJc8lxR"
    df = pd.read_csv(gdown.download(file_url, quiet=True), on_bad_lines='skip')

    # Afficher le contenu du DataFrame
    #print(df.head())
    #df = pd.read_csv(r'C:\Users\hicha\Desktop\AI-Resume-Classification\categorized_cvs.csv')


    # Suppression des lignes avec des valeurs manquantes
    df.dropna(subset=['category', 'text'], inplace=True)

    # Nettoyage avanc√© du texte
    df["clean_text"] = df["text"].apply(clean_text)

    # Encodage des cat√©gories
    label_encoder = LabelEncoder()
    df["category_encoded"] = label_encoder.fit_transform(df["category"])

    # Transformation du texte en vecteurs TF-IDF (bigrammes + 5000 features)
    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X = tfidf_vectorizer.fit_transform(df["clean_text"])
    y = df["category_encoded"]

    # Affichage de la distribution des classes avant l'√©quilibrage
    class_distribution_before = df['category'].value_counts().reset_index()
    class_distribution_before.columns = ['Category', 'Count']
    class_distribution_before['Status'] = 'Avant √©quilibrage'

    # √âquilibrage des classes sous-repr√©sent√©es avec RandomOverSampler
    ros = RandomOverSampler(random_state=42)
    X_resampled, y_resampled = ros.fit_resample(X, y)

    # Affichage de la distribution des classes apr√®s l'√©quilibrage
    class_distribution_after = pd.Series(y_resampled).value_counts().reset_index()
    class_distribution_after.columns = ['Category', 'Count']
    class_distribution_after['Status'] = 'Apr√®s √©quilibrage'

    # Fusionner les deux distributions pour comparaison
    class_distribution_comparison = pd.concat([class_distribution_before, class_distribution_after], axis=0)

    # Interface Streamlit
    st.title("Entra√Ænement des mod√®les de classification")

    # Affichage du tableau de comparaison des classes avant et apr√®s l'√©quilibrage
    #st.subheader("Comparaison de la distribution des classes avant et apr√®s √©quilibrage")
    #st.dataframe(class_distribution_comparison)

    # S√©paration des donn√©es en train/test (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

    # Fonction d'√©valuation des mod√®les
    def evaluate_model(name, model, X_test, y_test):
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        precision = report["macro avg"]["precision"]
        recall = report["macro avg"]["recall"]
        f1 = report["macro avg"]["f1-score"]
        
        return accuracy, precision, recall, f1, report

    # Section pour les hyperparam√®tres
    st.sidebar.header("Param√®tres des mod√®les")

    # Param√®tres pour Logistic Regression
    logreg_max_iter = st.sidebar.number_input("Max Iterations (Logistic Regression)", min_value=100, max_value=5000, value=2000)
    logreg_class_weight = st.sidebar.selectbox("Poids des classes (Logistic Regression)", ["balanced", "None"])

    # Param√®tres pour Random Forest
    rf_n_estimators = st.sidebar.selectbox("Nombre d'estimateurs (RandomForest)", [100, 200, 300])
    rf_max_depth = st.sidebar.selectbox("Profondeur maximale (RandomForest)", [10, 20, None])

    # Param√®tres pour SVM
    svm_kernel = st.sidebar.selectbox("Noyau (SVM)", ["linear", "rbf"])
    svm_class_weight = st.sidebar.selectbox("Poids des classes (SVM)", ["balanced", "None"])

    # Param√®tres pour KNeighbors
    knn_n_neighbors = st.sidebar.selectbox("Nombre de voisins (KNeighbors)", [3, 5, 7, 9])

    # Param√®tres pour GradientBoosting
    gb_n_estimators = st.sidebar.selectbox("Nombre d'estimateurs (Gradient Boosting)", [100, 200, 300])
    gb_learning_rate = st.sidebar.number_input("Taux d'apprentissage (Gradient Boosting)", min_value=0.01, max_value=1.0, value=0.1)

    # Param√®tres pour Naive Bayes
    nb_alpha = st.sidebar.number_input("Alpha (Naive Bayes)", min_value=0.1, max_value=2.0, value=1.0)

    # Variables pour stocker les r√©sultats
    results = {
        'Model': [],
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1-Score': []
    }

    best_model = None
    best_model_name = ""
    model_trained = False  # Flag pour suivre si les mod√®les ont √©t√© entra√Æn√©s

    # D√©finir un chemin local pour sauvegarder les mod√®les
    models_dir = r'C:\Users\hicha\Desktop\AI-Resume-Classification\Models' # Le dossier "Models" dans le r√©pertoire actuel

    # Bouton pour entra√Æner les mod√®les
    if st.sidebar.button("Entra√Æner les mod√®les"):
        # V√©rifier le r√©pertoire Models et le cr√©er s'il n'existe pas
        if not os.path.exists(models_dir):
            os.makedirs(models_dir)
            st.write(f"R√©pertoire '{models_dir}' cr√©√©.")
        else:
            st.write(f"R√©pertoire '{models_dir}' d√©j√† existant.")
        with st.spinner("Les mod√®les sont en cours d'entra√Ænement..."):
            # R√©gression logistique
            logreg_model = LogisticRegression(max_iter=logreg_max_iter, class_weight=logreg_class_weight)
            logreg_model.fit(X_train, y_train)
            acc_logreg, precision_logreg, recall_logreg, f1_logreg, report_logreg = evaluate_model("Logistic Regression", logreg_model, X_test, y_test)
            results['Model'].append('Logistic Regression')
            results['Accuracy'].append(acc_logreg)
            results['Precision'].append(precision_logreg)
            results['Recall'].append(recall_logreg)
            results['F1-Score'].append(f1_logreg)

            # Random Forest
            rf_model = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=rf_max_depth, random_state=42)
            rf_model.fit(X_train, y_train)
            acc_rf, precision_rf, recall_rf, f1_rf, report_rf = evaluate_model("Random Forest", rf_model, X_test, y_test)
            results['Model'].append('Random Forest')
            results['Accuracy'].append(acc_rf)
            results['Precision'].append(precision_rf)
            results['Recall'].append(recall_rf)
            results['F1-Score'].append(f1_rf)

            # SVM
            svm_model = SVC(kernel=svm_kernel, class_weight=svm_class_weight)
            svm_model.fit(X_train, y_train)
            acc_svm, precision_svm, recall_svm, f1_svm, report_svm = evaluate_model("SVM", svm_model, X_test, y_test)
            results['Model'].append('SVM')
            results['Accuracy'].append(acc_svm)
            results['Precision'].append(precision_svm)
            results['Recall'].append(recall_svm)
            results['F1-Score'].append(f1_svm)

            # KNeighbors
            knn_model = KNeighborsClassifier(n_neighbors=knn_n_neighbors)
            knn_model.fit(X_train, y_train)
            acc_knn, precision_knn, recall_knn, f1_knn, report_knn = evaluate_model("KNeighbors", knn_model, X_test, y_test)
            results['Model'].append('KNeighbors')
            results['Accuracy'].append(acc_knn)
            results['Precision'].append(precision_knn)
            results['Recall'].append(recall_knn)
            results['F1-Score'].append(f1_knn)

            # Gradient Boosting
            gb_model = GradientBoostingClassifier(n_estimators=gb_n_estimators, learning_rate=gb_learning_rate)
            gb_model.fit(X_train, y_train)
            acc_gb, precision_gb, recall_gb, f1_gb, report_gb = evaluate_model("Gradient Boosting", gb_model, X_test, y_test)
            results['Model'].append('Gradient Boosting')
            results['Accuracy'].append(acc_gb)
            results['Precision'].append(precision_gb)
            results['Recall'].append(recall_gb)
            results['F1-Score'].append(f1_gb)

            # Naive Bayes
            nb_model = MultinomialNB(alpha=nb_alpha)
            nb_model.fit(X_train, y_train)
            acc_nb, precision_nb, recall_nb, f1_nb, report_nb = evaluate_model("Naive Bayes", nb_model, X_test, y_test)
            results['Model'].append('Naive Bayes')
            results['Accuracy'].append(acc_nb)
            results['Precision'].append(precision_nb)
            results['Recall'].append(recall_nb)
            results['F1-Score'].append(f1_nb)

            # Cr√©ation du DataFrame des r√©sultats
            results_df = pd.DataFrame(results)

            # Affichage des r√©sultats sous forme de tableau
            st.subheader("Tableau des M√©triques des Mod√®les")
            st.dataframe(results_df)

            # Affichage des graphiques s√©par√©s pour chaque m√©trique

            # Graphique pour Accuracy
            #st.subheader("Pr√©cision (Accuracy)")
            #fig, ax = plt.subplots(figsize=(10, 5))
            #sns.barplot(x='Model', y='Accuracy', data=results_df, palette="viridis", ax=ax)
            #ax.set_title('Pr√©cision (Accuracy)')
            #st.pyplot(fig)

            # Graphique pour Precision
            #st.subheader("Pr√©cision (Precision)")
            #fig, ax = plt.subplots(figsize=(10, 5))
            #sns.barplot(x='Model', y='Precision', data=results_df, palette="viridis", ax=ax)
            #ax.set_title('Pr√©cision (Precision)')
            #st.pyplot(fig)

            # Graphique pour Recall
            #st.subheader("Rappel (Recall)")
            #fig, ax = plt.subplots(figsize=(10, 5))
            #sns.barplot(x='Model', y='Recall', data=results_df, palette="viridis", ax=ax)
            #ax.set_title('Rappel (Recall)')
            #st.pyplot(fig)

            # Graphique pour F1-Score
            #st.subheader("F1-Score")
            #fig, ax = plt.subplots(figsize=(10, 5))
            #sns.barplot(x='Model', y='F1-Score', data=results_df, palette="viridis", ax=ax)
            #ax.set_title('F1-Score')
            #st.pyplot(fig)

            # D√©terminer le meilleur mod√®le
            best_model_info = max([(logreg_model, "Logistic Regression", acc_logreg),
                                  (rf_model, "Random Forest", acc_rf),
                                  (svm_model, "SVM", acc_svm),
                                  (knn_model, "KNeighbors", acc_knn),
                                  (gb_model, "Gradient Boosting", acc_gb),
                                  (nb_model, "Naive Bayes", acc_nb)], key=lambda x: x[2])

            best_model_name = best_model_info[1]
            best_model = best_model_info[0]
            best_model_acc = best_model_info[2]

            st.write(f"**Meilleur mod√®le : {best_model_name} avec une pr√©cision de {best_model_acc:.4f}**")

            # Sauvegarde du meilleur mod√®le
            model_path = r'C:\Users\hicha\Desktop\AI-Resume-Classification\Models\best_model.pkl'
            tfidf_path = r'C:\Users\hicha\Desktop\AI-Resume-Classification\Models\tfidf_vectorizer.pkl'
            label_encoder_path = r'C:\Users\hicha\Desktop\AI-Resume-Classification\Models\label_encoder.pkl'


            try:
                st.write(f"D√©but de la sauvegarde du mod√®le √† {model_path}")
                joblib.dump(best_model, model_path)
                joblib.dump(tfidf_vectorizer, tfidf_path)
                joblib.dump(label_encoder, label_encoder_path)
                st.success(f"‚úÖ Mod√®le, TfidfVectorizer et LabelEncoder sauvegard√©s avec succ√®s.")
            except FileNotFoundError as fnf_error:
                st.error(f"‚ùå Erreur FileNotFoundError : {str(fnf_error)}")
            except PermissionError as perm_error:
                st.error(f"‚ùå Erreur PermissionError : {str(perm_error)}")
            except Exception as e:
                st.error(f"‚ùå Erreur inconnue lors de la sauvegarde du mod√®le : {str(e)}")


# --- Script 2: Automated CV Analysis and Job Match ---
def automated_cv_analysis():

    # T√©l√©charger les stopwords
    nltk.download("stopwords")
    # T√©l√©charger le mod√®le si n√©cessaire
    #os.system("python -m spacy download fr_core_news_sm")
    # Charger le mod√®le de langage fran√ßais de SpaCy
    #nlp = spacy.load("fr_core_news_sm")

    def extract_text(uploaded_file):
        # Obtenir le nom du fichier pour v√©rifier son extension
        file_name = uploaded_file.name.lower()

        if file_name.endswith(".pdf"):
            with pdfplumber.open(uploaded_file) as pdf:
                return " ".join([page.extract_text() for page in pdf.pages if page.extract_text()])
        elif file_name.endswith(".docx"):
            doc = docx.Document(uploaded_file)
            return " ".join([para.text for para in doc.paragraphs])
        else:
            return StringIO(uploaded_file.getvalue().decode("utf-8")).read()

    # üìå Fonction de nettoyage du texte
    def clean_text(text):
        text = text.lower()
        text = re.sub(r"\d+", "", text)  # Supprimer les chiffres
        text = re.sub(r"[^\w\s]", "", text)  # Supprimer la ponctuation
        text = " ".join([word for word in text.split() if word not in stopwords.words("french")])
        return text

    # üìå Fonction pour extraire les technologies du CV sans doublons et leur fr√©quence
    def extract_technologies_with_count(text):
        # Liste compl√®te des comp√©tences √† extraire
        tech_keywords = [
            # Data Engineer
            "Python", "SQL", "Spark", "AWS", "Kafka", "Airflow", "Snowflake", "Redshift", "Databricks", "Docker", "Kubernetes", "Jenkins", "ETL", "Pipeline",

            # Data Scientist
            "Machine Learning", "Deep Learning", "NLP", "Computer Vision", "TensorFlow", "PyTorch", "Keras", "Scikit-learn", "Transformers", "BERT", "LSTM", "GANs", "Reinforcement Learning",

            # Data Analyst
            "Tableau", "Power BI", "SQL", "Excel", "Looker", "Google Data Studio", "DAX", "Pandas", "Matplotlib", "Reporting", "ETL", "Dashboard", "Visualization",

            # DevOps
            "Docker", "Kubernetes", "Terraform", "CI/CD", "Ansible", "Jenkins", "Git", "Helm", "Prometheus", "Grafana", "ArgoCD", "Istio", "OpenShift", "Infrastructure as Code",

            # Backend Developer
            "Java", "Spring Boot", "Microservices", "Hibernate", "REST API", "JPA", "SQL", "NoSQL", "RabbitMQ", "Kafka", "GraphQL", "WebFlux", "Docker", "Kubernetes",

            # Frontend Developer
            "React", "Angular", "Vue.js", "JavaScript", "TypeScript", "HTML", "CSS", "Redux", "Next.js", "Nuxt.js", "Tailwind", "Material-UI", "Cypress", "Jest", "Storybook", "Webpack",

            # Mobile Developer
            "Swift", "Kotlin", "Flutter", "React Native", "Android", "iOS", "Jetpack Compose", "SwiftUI", "Objective-C", "Dart", "Mobile UI/UX", "GraphQL", "Firebase",

            # Big Data Engineer
            "Hadoop", "Hive", "Pig", "Scala", "Spark", "Presto", "Flink", "HBase", "Cassandra", "ElasticSearch", "MapReduce", "Delta Lake", "Kudu", "YARN", "Zookeeper",

            # Cybersecurity Analyst
            "Cybersecurity", "Penetration Testing", "SIEM", "IDS/IPS", "Firewall", "Ethical Hacking", "Kali Linux", "Metasploit", "OWASP", "Burp Suite", "Nmap", "Security Compliance",

            # ETL Developer
            "ETL", "Talend", "SSIS", "Informatica", "DataStage", "Azure Data Factory", "Apache Nifi", "Pentaho", "Snowflake", "SQL", "Data Warehouse", "OLAP",

            # Database Administrator
            "Oracle", "MySQL", "PostgreSQL", "MongoDB", "Redis", "Cassandra", "MariaDB", "CockroachDB", "Elasticsearch", "TimescaleDB", "SQL", "NoSQL", "Database Replication", "Indexing",

            # CRM Consultant
            "Salesforce", "SAP", "ERP", "CRM", "Dynamics 365", "Zoho CRM", "HubSpot", "Workday", "NetSuite", "ServiceNow", "Business Process Automation", "RPA",

            # Business Intelligence Analyst
            "Excel", "VBA", "R", "Power BI", "Tableau", "SAS", "QlikView", "SQL", "Alteryx", "ETL", "Reporting", "KPI", "Business Intelligence", "Dashboarding",

            # Cloud Engineer
            "GCP", "AWS", "Azure", "Terraform", "Ansible", "CloudFormation", "Kubernetes", "Lambda", "Serverless", "DevOps", "Cloud Security", "IAM", "Networking",

            # AI Researcher
            "Computer Vision", "OpenCV", "YOLO", "Deep Learning", "TensorFlow", "PyTorch", "GANs", "Image Segmentation", "Object Detection", "Image Processing",

            # Blockchain Developer
            "Blockchain", "Ethereum", "Smart Contracts", "Solidity", "Hyperledger", "Polkadot", "Binance Smart Chain", "DeFi", "dApps", "NFTs", "Consensus Mechanisms",

            # IoT Engineer
            "IoT", "MQTT", "Edge Computing", "Raspberry Pi", "Arduino", "LoRaWAN", "Zigbee", "Smart Devices", "Industrial IoT", "IoT Security",

            # Network Engineer
            "Networking", "TCP/IP", "BGP", "OSPF", "SDN", "Cisco", "Juniper", "Wireshark", "Routing", "Switching", "Network Security", "VLAN", "MPLS",

            # IT Project Manager
            "Project Management", "Agile", "Scrum", "PMP", "Kanban", "Jira", "Confluence", "SAFe", "Prince2", "Risk Management", "Stakeholder Management", "Product Ownership"
        ]
        
        # Cr√©er une expression r√©guli√®re pour chaque technologie (en √©chappant les espaces et les parenth√®ses)
        tech_regex = [re.escape(tech.lower()) for tech in tech_keywords]
        combined_regex = r'\b(' + r'|'.join(tech_regex) + r')\b'

        # Trouver toutes les correspondances dans le texte (insensible √† la casse)
        found_technologies = re.findall(combined_regex, text.lower())

        # Compter la fr√©quence des technologies, sans doublons
        tech_counts = Counter(found_technologies)

        # Retourner la liste des technologies avec leur fr√©quence sous forme de dictionnaire
        return dict(tech_counts)

    # üìå Fonction pour d√©tecter le niveau d'exp√©rience

    def detect_experience(cv_text):
        cv_text = cv_text.lower()

        # D√©tecter les ann√©es d'exp√©rience explicites
        match = re.search(r'(\d+)\s*(ans|years)', cv_text)
        years_of_experience = int(match.group(1)) if match else 0

        # Listes de mots-cl√©s pour classifier
        junior_keywords = ["d√©butant", "junior", "stage", "apprentissage", "assistant"]
        senior_keywords = ["confirm√©", "middle", "exp√©rience significative", "3 ans", "4 ans", "5 ans", "6 ans", "7 ans"]
        expert_keywords = ["lead", "manager", "expert", "architecte", "10 ans", "15 ans", "principal"]

        # D√©terminer le niveau bas√© sur les ann√©es d√©tect√©es
        if years_of_experience >= 8:
            return "Expert"
        elif 3 <= years_of_experience < 8:
            return "Senior"
        elif years_of_experience > 0:
            return "Junior"

        # Si aucune ann√©e n'est d√©tect√©e, on v√©rifie les mots-cl√©s suppl√©mentaires
        if any(word in cv_text for word in expert_keywords):
            return "Expert"
        elif any(word in cv_text for word in senior_keywords):
            return "Senior"
        elif any(word in cv_text for word in junior_keywords):
            return "Junior"

        # Par d√©faut, classer comme Junior si aucune correspondance n'est trouv√©e
        return "Junior"

    # üìå Fonction pour cr√©er un nuage de mots
    def generate_wordcloud(text):
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        st.pyplot(plt)

    # üìå Fonction pour g√©n√©rer un graphique radar des comp√©tences
    def plot_radar_chart(user_skills, ideal_profile):
        labels = list(set(user_skills.keys()).union(set(ideal_profile.keys())))
        user_values = [user_skills.get(skill, 0) for skill in labels]
        ideal_values = [ideal_profile.get(skill, 0) for skill in labels]
        
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(r=user_values, theta=labels, fill='toself', name='CV'))
        fig.add_trace(go.Scatterpolar(r=ideal_values, theta=labels, fill='toself', name='Profil Id√©al'))
        
        fig.update_layout(polar=dict(radialaxis=dict(visible=True)), showlegend=True)
        st.plotly_chart(fig)

    # üìå Fonction pour scraper l'offre d'emploi
    def scrape_job_description(url):
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            
            # Extraire les sections pertinentes
            description_section = soup.find('div', class_='job-description')
            qualifications_section = soup.find('div', class_='job-qualifications')
            criteria_section = soup.find('ul', class_='arrow-list')
            
            # Concatenation des sections
            description = ""
            if description_section:
                description += description_section.get_text(separator="\n", strip=True) + "\n"
            if qualifications_section:
                description += qualifications_section.get_text(separator="\n", strip=True) + "\n"
            if criteria_section:
                description += criteria_section.get_text(separator="\n", strip=True) + "\n"
            
            return description
        else:
            return ""
        
    # üìå Fonction pour comparer les technologies avec un r√©pertoire de CVs
    def compare_technologies_with_cvs(job_description, cvs_directory=r'C:\Users\hicha\Desktop\AI-Resume-Classification\BaseCVs'):
        # Extraire les technologies de l'offre d'emploi
        job_tech_counts = extract_technologies_with_count(job_description)

        # Initialisation des r√©sultats
        matching_cvs = []

        # Parcours des fichiers dans le r√©pertoire
        for cv_file in os.listdir(cvs_directory):
            cv_path = os.path.join(cvs_directory, cv_file)
            if cv_path.endswith(('.pdf', '.docx')):  # Accepter seulement les fichiers PDF et DOCX
                with open(cv_path, 'rb') as file:
                    cv_text = extract_text(file)
                    tech_counts = extract_technologies_with_count(cv_text)

                    # Comparer les technologies du CV avec celles de l'offre d'emploi
                    common_tech = set(tech_counts.keys()).intersection(set(job_tech_counts.keys()))
                    if common_tech:
                        # Calculer la similarit√© entre le CV et l'offre d'emploi
                        similarity_score = compare_texts(cv_text, job_description)
                        matching_cvs.append({
                            'cv_file': cv_file,
                            'common_tech': common_tech,
                            'tech_counts': tech_counts,
                            'job_tech_counts': job_tech_counts,
                            'similarity_score': similarity_score
                        })
        
        return matching_cvs
    
    # üìå Fonction pour enregistrer les r√©sultats dans un fichier
    def export_matching_cvs(matching_cvs, cvs_directory=r'C:\Users\hicha\Desktop\AI-Resume-Classification\BaseCVs', output_directory=r'C:\Users\hicha\Desktop\AI-Resume-Classification\Output'):
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)

        # Cr√©er un ensemble pour suivre les fichiers d√©j√† export√©s
        exported_files = set()

        # Exporter les CVs complets dans le r√©pertoire de sortie
        for match in matching_cvs:
            cv_file = match['cv_file']
            cv_path = os.path.join(cvs_directory, cv_file)

            # V√©rifier si le fichier a d√©j√† √©t√© export√©
            if cv_file not in exported_files:
                # Copier le CV original dans le r√©pertoire de sortie
                shutil.copy(cv_path, output_directory)
                
                # Ajouter le fichier au set des fichiers export√©s
                exported_files.add(cv_file)
                
                # Ajouter un fichier texte avec les technologies communes
                output_file_path = os.path.join(output_directory, f"match_{cv_file}.txt")
                with open(output_file_path, 'w', encoding='utf-8') as f:
                    f.write(f"Fichier CV: {cv_file}\n")
                    f.write(f"Technologies communes avec l'offre d'emploi: {', '.join(match['common_tech'])}\n")



    # üìå Fonction pour comparer les technologies d√©tect√©es avec l'offre d'emploi
    def compare_technologies_with_job(tech_counts, job_description):
        # Extraire les technologies de l'offre d'emploi
        job_tech_counts = extract_technologies_with_count(job_description)

        # Calculer le nombre de technologies communes entre le CV et l'offre d'emploi
        common_tech = set(tech_counts.keys()).intersection(set(job_tech_counts.keys()))
        common_tech_count = sum([min(tech_counts[tech], job_tech_counts[tech]) for tech in common_tech])

        return common_tech_count

    # üìå Fonction pour obtenir les mots les plus fr√©quents dans un texte
    def get_top_keywords(text, num_keywords=5):
        words = text.split()
        words = [word for word in words if word not in stopwords.words("french")]  # Supprimer les stopwords
        word_counts = Counter(words)
        return word_counts.most_common(num_keywords)

    # üìå Fonction pour cr√©er un nuage de mots √† partir des mots les plus fr√©quents
    def generate_top_keywords_wordcloud(text):
        # Extraire les 5 mots les plus fr√©quents
        top_keywords = get_top_keywords(text, num_keywords=5)
        
        # Cr√©er un dictionnaire avec les mots et leurs fr√©quences
        word_freq = dict(top_keywords)

        # G√©n√©rer le nuage de mots avec les fr√©quences
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(word_freq)

        # Affichage du nuage de mots
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        st.pyplot(plt)

    # üìå Fonction pour comparer les textes
    def compare_texts(text1, text2):
        # Vectorisation des textes
        vectorizer = CountVectorizer().fit_transform([text1, text2])
        cosine_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])
        
        return cosine_sim[0][0]

    # üìå Fonction pour pr√©dire la cat√©gorie et l'exp√©rience en utilisant le mod√®le pr√©alablement entra√Æn√©
    def predict_cv(cv_text):
        # Charger les mod√®les et autres objets depuis le r√©pertoire local
        model_category = joblib.load(r"C:\Users\hicha\Desktop\AI-Resume-Classification\Models\best_model.pkl")
        vectorizer = joblib.load(r"C:\Users\hicha\Desktop\AI-Resume-Classification\Models\tfidf_vectorizer.pkl")
        label_encoder = joblib.load(r"C:\Users\hicha\Desktop\AI-Resume-Classification\Models\label_encoder.pkl")

        # Traitement du texte du CV
        cleaned_text = clean_text(cv_text)
        features = vectorizer.transform([cleaned_text])

        # Pr√©diction de la cat√©gorie et de l'exp√©rience
        predicted_category_encoded = model_category.predict(features)[0]
        predicted_category = label_encoder.inverse_transform([predicted_category_encoded])[0]
        predicted_experience = detect_experience(cv_text)

        return predicted_category, predicted_experience

    # üìå Interface Streamlit
    st.title("üìÑü§ñ CV Analysis & Job Match")
    st.subheader("Chargez un CV et obtenez la pr√©diction du m√©tier et de l'exp√©rience.")

    uploaded_file = st.file_uploader("üì§ Importer un fichier (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])

    # üî• L'utilisateur peut d√©finir son propre profil id√©al
    st.subheader("üîß D√©finissez votre profil id√©al")
    available_skills = ["Python","SQL","Sql","Spark","AWS","Kafka","Airflow","Snowflake","Redshift","Databricks","Docker","Kubernetes","Jenkins","ETL","Pipeline","Machine Learning",
            "Deep Learning","NLP","Computer Vision","TensorFlow","PyTorch","Keras","Scikit-learn","Transformers","BERT","LSTM","GANs","Reinforcement Learning","Tableau",
            "Power BI","Excel","Looker","Google Data Studio","DAX","Pandas","Matplotlib","Reporting","Dashboard","Visualization","Terraform","CI/CD","Ansible","Git","Helm","Prometheus","Grafana","ArgoCD","Istio",
            "OpenShift","Infrastructure as Code","Java","Spring Boot","Microservices","Hibernate","REST API","JPA","NoSQL","RabbitMQ","GraphQL","WebFlux","React","Angular","Vue.js","JavaScript","TypeScript","HTML","CSS","Redux","Next.js","Nuxt.js","Tailwind",
            "Material-UI","Cypress","Jest","Storybook","Webpack","Swift","Kotlin","Flutter","React Native","Android","iOS","Jetpack Compose","SwiftUI","Objective-C","Dart",
            "Mobile UI/UX","Firebase","Hadoop","Hive","Pig","Scala","Presto","Flink","HBase","Cassandra","ElasticSearch","MapReduce","Delta Lake","Kudu","YARN","Zookeeper","Cybersecurity","Penetration Testing","SIEM","IDS/IPS","Firewall","Ethical Hacking","Kali Linux","Metasploit","OWASP",
            "Burp Suite","Nmap","Security Compliance","Talend","SSIS","Informatica","DataStage","Azure Data Factory","Apache Nifi","Pentaho",
            "Data Warehouse","OLAP","Oracle","MySQL","PostgreSQL","MongoDB","Redis","MariaDB","CockroachDB","Elasticsearch","TimescaleDB",
            "Database Replication","Indexing","Salesforce","SAP","ERP","CRM","Dynamics 365","Zoho CRM","HubSpot","Workday","NetSuite","ServiceNow",
            "Business Process Automation","RPA","VBA","R","SAS","QlikView","Alteryx","KPI","Business Intelligence","Dashboarding","GCP","Azure","CloudFormation","Lambda","Serverless","DevOps","Cloud Security","IAM","Networking","OpenCV","YOLO",
            "Image Segmentation","Object Detection","Image Processing","Blockchain","Ethereum","Smart Contracts","Solidity","Hyperledger","Polkadot",
            "Binance Smart Chain","DeFi","dApps","NFTs","Consensus Mechanisms","IoT","MQTT","Edge Computing","Raspberry Pi","Arduino","LoRaWAN","Zigbee","Smart Devices",
            "Industrial IoT","IoT Security","TCP/IP","BGP","OSPF","SDN","Cisco","Juniper","Wireshark","Routing","Switching","Network Security","VLAN","MPLS",
            "Project Management","Agile","Scrum","PMP","Kanban","Jira","Confluence","SAFe","Prince2","Risk Management","Stakeholder Management","Product Ownership"]

    selected_skills = st.multiselect("S√©lectionnez les comp√©tences cl√©s :", available_skills)
    ideal_profile = {skill: 1 for skill in selected_skills}

    job_url = st.text_input("Entrez l'URL de l'offre d'emploi :", "")

    # Exemple d'URL √† afficher sous le champ de saisie (non cliquable)
    st.markdown("""
    *Exemple d'URL de la plateforme Emploi.ma :*  
    https://www.emploi.ma/offre-emploi-maroc/senior-data-engineers-hf-casablanca-rabat-8802045
    """)

    if uploaded_file:
        with st.spinner("üîç Analyse en cours..."):
            cv_text = extract_text(uploaded_file)
            cleaned_cv_text = clean_text(cv_text)

            st.subheader("üîç Contenu du CV :")
            st.text_area("Texte extrait :", cv_text, height=200)

            category, experience = predict_cv(cv_text)
            st.subheader(f"üéØ M√©tier : **{category}**")

            # Extraction des technologies et comptage
            tech_counts = extract_technologies_with_count(cv_text)

            if tech_counts:
                st.subheader("üõ†Ô∏è Technologies d√©tect√©es et leurs fr√©quences :")
                
                # Trier les technologies par fr√©quence (du plus √©lev√© au plus bas)
                sorted_tech_counts = sorted(tech_counts.items(), key=lambda x: x[1], reverse=True)
                
                # Prendre uniquement les 10 premiers √©l√©ments
                top_10_tech = sorted_tech_counts[:10]

                # Appliquer la transformation pour mettre la premi√®re lettre en majuscule
                top_10_tech = [(tech.capitalize(), count) for tech, count in top_10_tech]
                
                # Affichage sous forme de tableau
                tech_data = [{"Technologie": tech, "Fr√©quence": count} for tech, count in top_10_tech]
                st.table(tech_data)
                
                # Affichage avec un graphique √† barres
                tech_names = [tech for tech, count in top_10_tech]
                tech_frequencies = [count for tech, count in top_10_tech]
                
                
                # Cr√©er un graphique √† barres
                #plt.figure(figsize=(10, 6))
                #sns.barplot(x=tech_names, y=tech_frequencies, palette="viridis")
                #plt.title("Top 10 des technologies d√©tect√©es")
                #plt.xlabel("Technologies")
                #plt.ylabel("Fr√©quence")
                #plt.xticks(rotation=45, ha="right")
                
                #st.pyplot(plt)
                
            else:
                st.subheader("üõ†Ô∏è Aucune technologie d√©tect√©e")

            # üî• Ajout du graphique radar
            st.subheader("üìä Visualisation des comp√©tences")

            # Prendre uniquement les 10 technologies les plus fr√©quentes
            top_10_tech = sorted(tech_counts.items(), key=lambda x: x[1], reverse=True)[:10]

            # Appliquer la transformation pour mettre la premi√®re lettre en majuscule
            top_10_tech = [(tech.capitalize(), count) for tech, count in top_10_tech]
            
            user_skills = {tech: count for tech, count in top_10_tech}  # Utiliser seulement les 10 meilleures comp√©tences

            # Visualisation avec le graphique radar
            plot_radar_chart(user_skills, ideal_profile)

            # Extraction des technologies et comptage
            tech_counts = extract_technologies_with_count(cv_text)

            # V√©rification si l'URL de l'offre d'emploi est renseign√©e
            if job_url:
                # Scraper l'offre d'emploi
                job_description = scrape_job_description(job_url)

                if job_description:
                    st.subheader("üîç Comparaison des technologies de l'offre avec les CVs :")
                    
                    # Comparer les technologies du CV avec celles des CVs dans le r√©pertoire
                    matching_cvs = compare_technologies_with_cvs(job_description)

                    if matching_cvs:
                        # Afficher un tableau avec les r√©sultats
                        st.write("Technologies communes trouv√©es dans les CVs :")
                        data = []
                        for match in matching_cvs:
                            data.append({
                                "Nom du CV": match["cv_file"],
                                "Technologies communes": ", ".join(match["common_tech"]),
                                "Score de Similarit√©": round(match["similarity_score"], 2)
                            })
                        # Afficher le tableau avec les informations
                        #st.write(pd.DataFrame(data))
                        # Afficher un tableau avec les r√©sultats
                        st.write(pd.DataFrame(data).sort_values(by="Score de Similarit√©", ascending=False))


                        # Exporter les CVs originaux et les informations dans un r√©pertoire
                        export_matching_cvs(matching_cvs)

                        # Afficher un message indiquant que les r√©sultats ont √©t√© export√©s
                        st.success(f"Les CVs originaux ont √©t√© export√©s dans le dossier")
                    else:
                        st.write("Aucune technologie commune trouv√©e dans les CVs.")
                else:
                    st.error("Erreur lors du scraping de l'offre d'emploi.")
            else:
                st.write("Aucune URL d'offre d'emploi fournie. Pas de comparaison effectu√©e.")


def set_sidebar_style():
    st.markdown("""
    <style>
        /* Style pour la barre lat√©rale */
        .sidebar .sidebar-content {
            background-color: #000000; /* Arri√®re-plan noir pour la sidebar */
            color: white;
        }

        /* Style du titre dans la sidebar */
        .sidebar .sidebar-content h1, .sidebar .sidebar-content h2 {
            color: #ffffff;
            font-family: 'Arial', sans-serif;
        }

        /* Style des radio buttons */
        .stRadio>div {
            background-color: #333333; /* Fond gris fonc√© pour les boutons */
            color: white; 
            border: 2px solid #ffffff;  /* Bordure blanche */
            border-radius: 8px;
            padding: 12px;
            font-size: 18px;
            font-weight: bold;
            text-align: center;
            margin-bottom: 10px;
        }

        /* Style quand le bouton est survol√© */
        .stRadio>div:hover {
            background-color: #555555; /* Changer la couleur de fond au survol */
        }

        /* Style du texte dans les boutons radio */
        .stRadio>div>label {
            color: white; /* Texte blanc pour les labels */
        }
    </style>
    """, unsafe_allow_html=True)

# --- Fonction principale avec les onglets stylis√©s ---
def main():
    set_sidebar_style()  # Appliquer les styles

    st.sidebar.title("Navigation")

    # Utiliser des boutons radio pour les onglets (afin de mieux les styliser)
    option = st.sidebar.radio(
        "Choisissez un onglet",
        ("Resume Classification", "CV Analysis and Job Match"),
        index=0,  # Index de l'onglet par d√©faut
        key="sidebar_radio"
    )

    # S√©curiser l'option "Resume Classification"
    if option == "Resume Classification":
        # Demander l'authentification avant d'acc√©der √† cette option
        if authenticate_user():
            resume_classification()  # Si authentification r√©ussie, afficher le contenu
        else:
            st.title("üìÇ‚ú® Smart Resume Classification")
            st.write("Acc√®s non autoris√©. Veuillez entrer vos informations de connexion.")
    
    elif option == "CV Analysis and Job Match":
        automated_cv_analysis()  # Cette option reste publique

if __name__ == "__main__":
    main()
