# -*- coding: utf-8 -*-
"""AI-Resume Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1icNTSJRi6vJf5_mn-0-i-LeawKvoC82d
"""

!pip install streamlit -q
!pip install streamlit pandas scikit-learn joblib wordcloud nltk spacy pdfplumber python-docx
!python -m spacy download fr_core_news_sm
!pip install PyMuPDF
!pip install requests beautifulsoup4 spacy sklearn
!python -m spacy download en_core_web_sm
!pip install streamlit requests beautifulsoup4 spacy sklearn PyPDF2 python-docx
!pip install PyPDF2
!pip install selenium

import os
import pandas as pd
import fitz  # PyMuPDF
import docx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Fonction pour extraire le texte d'un fichier PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Fonction pour extraire le texte d'un fichier Word (docx)
def extract_text_from_word(word_path):
    doc = docx.Document(word_path)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    return text

# Fonction pour extraire le texte d'un fichier (PDF ou Word) avec gestion d'erreur
def extract_text(file_path):
    try:
        if file_path.endswith('.pdf'):
            return extract_text_from_pdf(file_path)
        elif file_path.endswith('.docx'):
            return extract_text_from_word(file_path)
        else:
            return ""
    except Exception as e:
        print(f"Erreur lors de l'extraction du fichier {file_path}: {e}")
        return ""  # Retourne une chaîne vide si une erreur se produit

# Fonction principale pour traiter les CV et générer les catégories
def categorize_cvs(dataframe, cv_directory, output_csv_path):
    # Vectorizer pour transformer les textes en vecteurs TF-IDF
    vectorizer = TfidfVectorizer(stop_words='english')

    # Étape 1 : Vectoriser les textes de la DataFrame (text) pour comparaison
    df_texts = dataframe['text'].values
    tfidf_matrix = vectorizer.fit_transform(df_texts)

    # Liste pour stocker les résultats
    results = []

    # Étape 2 : Parcourir tous les CV dans le répertoire
    for filename in os.listdir(cv_directory):
        file_path = os.path.join(cv_directory, filename)
        if os.path.isfile(file_path):
            # Extraire le texte du CV
            cv_text = extract_text(file_path)

            if cv_text:  # Si le CV contient du texte
                # Étape 3 : Vectoriser le texte du CV
                cv_vector = vectorizer.transform([cv_text])

                # Étape 4 : Calculer la similarité entre le CV et tous les textes dans la DataFrame
                similarities = cosine_similarity(cv_vector, tfidf_matrix)

                # Étape 5 : Trouver la catégorie avec la plus grande similarité
                best_match_index = similarities.argmax()
                best_category = dataframe.iloc[best_match_index]['category']

                # Ajouter les résultats dans la liste
                results.append({'cv_filename': filename, 'category': best_category, 'text': cv_text})

    # Créer un DataFrame avec les résultats
    result_df = pd.DataFrame(results)

    # Étape 6 : Exporter le DataFrame vers un fichier CSV
    result_df.to_csv(output_csv_path, index=False, encoding='utf-8')
    print(f"Les résultats ont été exportés vers : {output_csv_path}")

# Exemple d'utilisation
if __name__ == "__main__":
    # Charger ta DataFrame spécifique avec les catégories
    data = [
        ("Python SQL Spark AWS Kafka Airflow Snowflake Redshift Databricks Docker Kubernetes Jenkins ETL Pipeline", "Data Engineer"),
        ("Machine Learning Deep Learning NLP Computer Vision TensorFlow PyTorch Keras Scikit-learn Transformers BERT LSTM GANs Reinforcement Learning", "Data Scientist"),
        ("Tableau Power BI SQL Excel Looker Google Data Studio DAX Pandas Matplotlib Reporting ETL Dashboard Visualization", "Data Analyst"),
        ("Docker Kubernetes Terraform CI/CD Ansible Jenkins Git Helm Prometheus Grafana ArgoCD Istio OpenShift Infrastructure as Code", "DevOps"),
        ("Java Spring Boot Microservices Hibernate REST API JPA SQL NoSQL RabbitMQ Kafka GraphQL WebFlux Docker Kubernetes", "Backend Developer"),
        ("React Angular Vue.js JavaScript TypeScript HTML CSS Redux Next.js Nuxt.js Tailwind Material-UI Cypress Jest Storybook Webpack", "Frontend Developer"),
        ("Swift Kotlin Flutter React Native Android iOS Jetpack Compose SwiftUI Objective-C Dart Mobile UI/UX GraphQL Firebase", "Mobile Developer"),
        ("Hadoop Hive Pig Scala Spark Presto Flink HBase Cassandra ElasticSearch MapReduce Delta Lake Kudu YARN Zookeeper", "Big Data Engineer"),
        ("Cybersecurity Penetration Testing SIEM IDS/IPS Firewall Ethical Hacking Kali Linux Metasploit OWASP Burp Suite Nmap Security Compliance", "Cybersecurity Analyst"),
        ("ETL Talend SSIS Informatica DataStage Azure Data Factory Apache Nifi Pentaho Snowflake SQL Data Warehouse OLAP", "ETL Developer"),
        ("Oracle MySQL PostgreSQL MongoDB Redis Cassandra MariaDB CockroachDB Elasticsearch TimescaleDB SQL NoSQL Database Replication Indexing", "Database Administrator"),
        ("Salesforce SAP ERP CRM Dynamics 365 Zoho CRM HubSpot Workday NetSuite ServiceNow Business Process Automation RPA", "CRM Consultant"),
        ("Excel VBA R Power BI Tableau SAS QlikView SQL Alteryx ETL Reporting KPI Business Intelligence Dashboarding", "Business Intelligence Analyst"),
        ("GCP AWS Azure Terraform Ansible CloudFormation Kubernetes Lambda Serverless DevOps Cloud Security IAM Networking", "Cloud Engineer"),
        ("Computer Vision OpenCV YOLO Deep Learning TensorFlow PyTorch GANs Image Segmentation Object Detection Image Processing", "AI Researcher"),
        ("Blockchain Ethereum Smart Contracts Solidity Hyperledger Polkadot Binance Smart Chain DeFi dApps NFTs Consensus Mechanisms", "Blockchain Developer"),
        ("IoT MQTT Edge Computing Raspberry Pi Arduino LoRaWAN Zigbee Smart Devices Industrial IoT IoT Security", "IoT Engineer"),
        ("Networking TCP/IP BGP OSPF SDN Cisco Juniper Wireshark Routing Switching Network Security VLAN MPLS", "Network Engineer"),
        ("Project Management Agile Scrum PMP Kanban Jira Confluence SAFe Prince2 Risk Management Stakeholder Management Product Ownership", "IT Project Manager"),
    ]

    df = pd.DataFrame(data, columns=["text", "category"])

    # Répertoire contenant les CV
    cv_directory = '/content/drive/MyDrive/Vos coordonnées (File responses)/Curriculum Vitae (File responses)'  # Remplace par le chemin réel

    # Chemin pour enregistrer le fichier CSV des résultats
    output_csv_path = '/content/drive/MyDrive/categorized_cvs.csv'  # Remplace par le chemin où tu veux sauvegarder le fichier CSV

    # Obtenir les catégories pour les CV et exporter vers CSV
    categorize_cvs(df, cv_directory, output_csv_path)

import pandas as pd

# Charger ton DataFrame
df = pd.read_csv('/content/drive/MyDrive/categorized_cvs.csv')  # Remplace avec le bon chemin

# Compter le nombre de CVs par catégorie
category_counts = df['category'].value_counts()

# Afficher le résultat
df

import pandas as pd

# Charger ton DataFrame
df = pd.read_csv('/content/drive/MyDrive/categorized_cvs.csv')  # Remplace avec le bon chemin

# Compter le nombre de CVs par catégorie
category_counts = df['category'].value_counts()

# Afficher le résultat
print(category_counts)

!wget -q -O - ipv4.icanhazip.com

! streamlit run /content/drive/MyDrive/Home.py & npx localtunnel --port 8501